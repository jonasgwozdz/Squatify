{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import tempfile\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# Third-party imports for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Third-party imports for machine learning\n",
    "import joblib\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Third-party imports for data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Third-party imports for video processing and computer vision\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# IPython imports for interactive widgets and display utilities\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Video, HTML, display, clear_output\n",
    "\n",
    "# Local imports for video processing pipeline\n",
    "from src.pipeline.tracking.video_processing_pipeline import process_video, body_parts\n",
    "\n",
    "# Configuration\n",
    "tracking_data_dir = './src/pipeline/tracking_data'\n",
    "output_video_dir = './src/pipeline/output_videos'\n",
    "predictions_path = \"./src/pipeline/predictions\"\n",
    "models_dir = './data/models'\n",
    "labeled_data_path = './data/training/labled_data'\n",
    "tracking_data_path = '' # will be set by video tracking cell, otherwise define csv file here\n",
    "\n",
    "# Ensure all directories exist\n",
    "dirs_to_check = [tracking_data_dir, output_video_dir]\n",
    "for dir in dirs_to_check:\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "        print(f\"Directory created: {dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file upload widget\n",
    "file_selector = widgets.FileUpload(\n",
    "    accept='.mp4, .mov',  # Specify file types\n",
    "    multiple=False,  # Allow multiple files to be selected\n",
    "    description='Select a video file',\n",
    "    layout={'width': '400px'}\n",
    ")\n",
    "\n",
    "# Define a button widget\n",
    "button = widgets.Button(\n",
    "    description='Process Uploaded File',\n",
    "    button_style='',  # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click to process the uploaded file',\n",
    "    icon='play',  # Button icon\n",
    "    layout={'width': '400px'}\n",
    ")\n",
    "\n",
    "def save_uploaded_file(uploaded_file):\n",
    "    file_name = uploaded_file['name']\n",
    "    file_content = uploaded_file['content']\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=file_name) as tmp_file:\n",
    "        tmp_file.write(file_content)\n",
    "        return tmp_file.name, os.path.dirname(tmp_file.name)\n",
    "\n",
    "def save_landmarks_on_video(video_path, output_video_path):\n",
    "    print(\"Creating video with landmarks... (this might take a while)\")\n",
    "\n",
    "    # Initialize MediaPipe Pose\n",
    "    mp_pose = mp.solutions.pose\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Check if video opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "\n",
    "    # Get the width, height, and frame rate of the video frame\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Read the tracking data from CSV\n",
    "    tracking_data = pd.read_csv(tracking_data_path)\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'avc1')  # or 'XVID'\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Process video and draw landmarks from tracking data\n",
    "    for index, row in tracking_data.iterrows():\n",
    "        # Read frame from video\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Draw landmarks on the frame\n",
    "        for landmark in mp.solutions.pose.PoseLandmark:\n",
    "            landmark_name = landmark.name.lower().replace('_', ' ')  # Convert to lowercase and replace underscores with spaces\n",
    "            landmark_name = landmark_name.capitalize()  # Capitalize only the first letter of the first word\n",
    "            landmark_x = f'{landmark_name}_x'\n",
    "            landmark_y = f'{landmark_name}_y'\n",
    "            if landmark_x in tracking_data.columns and landmark_y in tracking_data.columns and not pd.isna(row[landmark_x]) and not pd.isna(row[landmark_y]):\n",
    "                x = int(row[landmark_x] * width)\n",
    "                y = int(row[landmark_y] * height)\n",
    "                # Check if the current landmark is one of the hips\n",
    "                if landmark in [mp.solutions.pose.PoseLandmark.LEFT_HIP, mp.solutions.pose.PoseLandmark.RIGHT_HIP]:\n",
    "                    # Highlight hips in red\n",
    "                    cv2.circle(frame, (x, y), 15, (0, 0, 255), -1)\n",
    "                else:\n",
    "                    # Draw other landmarks in green\n",
    "                    cv2.circle(frame, (x, y), 5, (0, 255, 0), -1)\n",
    "\n",
    "\n",
    "        # Write the frame with landmarks to the output video file\n",
    "        out.write(frame)\n",
    "\n",
    "    # Release everything when job is finished\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"Video saved to {output_video_path}\")\n",
    "\n",
    "def process_and_display_tracking_data(tmp_file_path, tmp_file_dir):\n",
    "    print(\"Processing video... (this might take a while)\")\n",
    "    process_video(os.path.basename(tmp_file_path), tmp_file_dir, tracking_data_dir)\n",
    "    tracking_data_file = f'Tracking_{os.path.basename(tmp_file_path[:-4])}.csv'\n",
    "    tracking_data_df = pd.read_csv(os.path.join(tracking_data_dir, tracking_data_file))\n",
    "    display(tracking_data_df.head())\n",
    "    # After tracking data is processed, save video with landmarks\n",
    "    output_video_path = os.path.join(output_video_dir, f\"{os.path.basename(tmp_file_path[:-4])}_landmarks.mp4\")\n",
    "    global tracking_data_path\n",
    "    tracking_data_path = os.path.join(tracking_data_dir, tracking_data_file)\n",
    "    save_landmarks_on_video(tmp_file_path, output_video_path)\n",
    "    video_html = f'''\n",
    "    Video with landmarks saved to \n",
    "    <a href='{output_video_path}' target='_blank'>{output_video_path}</a> <br><br>\n",
    "    <video height=\"500\" controls allowfullscreen>\n",
    "      <source src=\"{output_video_path}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "    </video>\n",
    "    '''\n",
    "    display(HTML(video_html))\n",
    "\n",
    "# Define an event handler for the button click event\n",
    "def on_button_clicked(b):\n",
    "    uploaded_files = file_selector.value\n",
    "    if uploaded_files:\n",
    "        uploaded_file = next(iter(uploaded_files))\n",
    "        tmp_file_path, tmp_file_dir = save_uploaded_file(uploaded_file)\n",
    "        process_and_display_tracking_data(tmp_file_path, tmp_file_dir)\n",
    "    else:\n",
    "        print(\"No file uploaded.\")\n",
    "\n",
    "# Attach the event handler to the button\n",
    "button.on_click(on_button_clicked)\n",
    "\n",
    "# Display the widgets\n",
    "display(file_selector, button)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Usage - Squat Phase Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analysing Squat Phase of Tracked Data: \" + tracking_data_path)\n",
    "\n",
    "def plot_movement_data(csv_file_path, title=\"Average Hip Height over Time\"):\n",
    "    # Read the data from the CSV file\n",
    "    data = pd.read_csv(csv_file_path, delimiter=\",\")\n",
    "\n",
    "    # Calculate the average hip height\n",
    "    data[\"Average_Hip_Height\"] = (data[\"Left hip_y\"] + data[\"Right hip_y\"]) / 2\n",
    "\n",
    "    # Create a scatter plot\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Check if 'Label' column exists and plot accordingly\n",
    "    if 'Label' in data.columns and data['Label'].notna().any():\n",
    "        # Define custom color mapping for movement types\n",
    "        custom_color_mapping = {\n",
    "            \"Pause\": \"orange\",\n",
    "            \"Ascending\": \"green\",\n",
    "            \"Descending\": \"red\",\n",
    "            \"Transition\": \"yellow\",\n",
    "            \"Unknown\": \"grey\",\n",
    "        }\n",
    "\n",
    "        for label, color in custom_color_mapping.items():\n",
    "            mask = data[\"Label\"] == label\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=data.loc[mask, \"Timestamp\"],\n",
    "                    y=data.loc[mask, \"Average_Hip_Height\"],\n",
    "                    mode=\"markers\",\n",
    "                    name=label,\n",
    "                    marker_color=color,\n",
    "                    hovertemplate=\"x: %{x}\",\n",
    "                )\n",
    "            )\n",
    "    else:\n",
    "        # Plot the average hip height over time in blue if no 'Label' column\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=data[\"Timestamp\"],\n",
    "                y=data[\"Average_Hip_Height\"],\n",
    "                mode=\"markers\",\n",
    "                name=\"Average Hip Height\",\n",
    "                marker_color=\"blue\",\n",
    "                hovertemplate=\"x: %{x}\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Get the x values of the first and last data points\n",
    "    first_x = data[\"Timestamp\"].iloc[0]\n",
    "    last_x = data[\"Timestamp\"].iloc[-1]\n",
    "\n",
    "    # Add vertical lines at the first and last data points\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=first_x,\n",
    "        y0=0,\n",
    "        x1=first_x,\n",
    "        y1=1,\n",
    "        yref=\"paper\",\n",
    "        line=dict(color=\"Green\", width=2),\n",
    "    )\n",
    "    fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=last_x,\n",
    "        y0=0,\n",
    "        x1=last_x,\n",
    "        y1=1,\n",
    "        yref=\"paper\",\n",
    "        line=dict(color=\"Red\", width=2),\n",
    "    )\n",
    "\n",
    "    # Add annotations for the first and last data points\n",
    "    fig.add_annotation(\n",
    "        x=first_x, y=0.05, text=f\"Start: {first_x}\", showarrow=False, yref=\"paper\"\n",
    "    )\n",
    "    fig.add_annotation(\n",
    "        x=last_x, y=0.05, text=f\"End: {last_x}\", showarrow=False, yref=\"paper\"\n",
    "    )\n",
    "\n",
    "    # Increase the number of grid lines\n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor=\"LightGrey\")\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor=\"LightGrey\", autorange=\"reversed\")\n",
    "\n",
    "    # Remove the light blue background\n",
    "    fig.update_layout(\n",
    "        autosize=True,  \n",
    "        height=600,  \n",
    "        hovermode=\"x\", \n",
    "        plot_bgcolor=\"rgba(0,0,0,0)\",\n",
    "        paper_bgcolor=\"rgba(0,0,0,0)\",\n",
    "        title=f\"{title}\"  \n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "# Define a blacklist of models to exclude\n",
    "blacklist = ['imputer.pkl']  \n",
    "\n",
    "# Get all the folders from the models directory excluding the \"old\" folder\n",
    "model_folders = [f.path for f in os.scandir(models_dir) if f.is_dir() and os.path.basename(f.path) != \"old\"]\n",
    "\n",
    "# Dropdown for Folder Selection\n",
    "folder_dropdown = widgets.Dropdown(\n",
    "    options=[(os.path.basename(folder), folder) for folder in model_folders],\n",
    "    description='Folder:',\n",
    "    disabled=False,\n",
    "    value=None,\n",
    ")\n",
    "\n",
    "# Dropdown for Model Selection, initially empty\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=[],\n",
    "    description='Model:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Function to update model dropdown based on folder selection\n",
    "def update_model_dropdown(change):\n",
    "    folder_path = change['new']\n",
    "    model_files = [f for f in os.listdir(folder_path) if f.endswith('.pkl') and f not in blacklist]\n",
    "    model_dropdown.options = [(file, os.path.join(folder_path, file)) for file in model_files]\n",
    "\n",
    "# Watch for changes in folder selection to update models dropdown\n",
    "folder_dropdown.observe(update_model_dropdown, names='value')\n",
    "\n",
    "# Button for Starting Analysis\n",
    "start_analysis_button = widgets.Button(\n",
    "    description='Start Analysis',\n",
    "    button_style='info',\n",
    "    tooltip='Click to start the analysis with the selected model',\n",
    "    icon='play'\n",
    ")\n",
    "\n",
    "# Display the widgets\n",
    "display(folder_dropdown, model_dropdown, start_analysis_button)\n",
    "\n",
    "# Function to create sequences from data\n",
    "def create_sequences(features, window_size):\n",
    "    feature_sequences = []\n",
    "    for i in range(len(features) - window_size + 1):\n",
    "        feature_sequences.append(features.iloc[i:(i + window_size)].values)\n",
    "    return np.array(feature_sequences)\n",
    "\n",
    "# Event Handler for Analysis Button\n",
    "def on_analysis_button_clicked(b):\n",
    "    # Clear the previous outputs\n",
    "    clear_output(wait=True)\n",
    "    display(folder_dropdown, model_dropdown, start_analysis_button)\n",
    "    \n",
    "    selected_model_path = model_dropdown.value\n",
    "    print(\"Starting analysis with model:\", selected_model_path)\n",
    "    \n",
    "    # Load the tracking data\n",
    "    new_data = pd.read_csv(tracking_data_path)\n",
    "\n",
    "    # Prepare the data as per the model's training conditions\n",
    "    new_data['Left hip_y_diff'] = new_data['Left hip_y'].diff().fillna(0)\n",
    "    new_data['Right hip_y_diff'] = new_data['Right hip_y'].diff().fillna(0)\n",
    "\n",
    "    # If the new data includes the 'Label' column, drop it\n",
    "    if \"Label\" in new_data.columns:\n",
    "        new_data = new_data.drop(\"Label\", axis=1)\n",
    "\n",
    "    required_features = ['Left hip_y', 'Right hip_y']\n",
    "    new_data_for_sequences = new_data[required_features]\n",
    "\n",
    "    # Create sequences from the new data\n",
    "    # Extract the window size from the directory name\n",
    "    match = re.search(r'sequence-(\\d+)f', selected_model_path)\n",
    "    if match:\n",
    "        window_size = int(match.group(1))\n",
    "    else:\n",
    "        window_size = 4  # Default value if no match is found\n",
    "    new_data_sequences = create_sequences(new_data_for_sequences, window_size)\n",
    "\n",
    "    # Flatten the sequences for compatibility with traditional models\n",
    "    new_data_flat = new_data_sequences.reshape(new_data_sequences.shape[0], -1)\n",
    "\n",
    "    # Load the fitted imputer if needed and apply it to the flattened new data\n",
    "    imputer_path = os.path.join(models_dir, 'imputer.pkl')\n",
    "    if os.path.exists(imputer_path):\n",
    "        imputer = joblib.load(imputer_path)\n",
    "        new_data_imputed = imputer.transform(new_data_flat)\n",
    "    else:\n",
    "        new_data_imputed = new_data_flat\n",
    "\n",
    "    # Load the selected model and make predictions\n",
    "    loaded_model = joblib.load(selected_model_path)\n",
    "    predictions = loaded_model.predict(new_data_imputed)\n",
    "\n",
    "    # Handle predictions (e.g., assigning predictions to the last element of each sequence)\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        if i + window_size - 1 < len(new_data):\n",
    "            new_data.at[i + window_size - 1, 'Label'] = prediction\n",
    "\n",
    "    # Save the DataFrame with the new labels back to a CSV file in the new data subfolder\n",
    "    predicted_file_name = f\"{os.path.basename(tracking_data_path[:-4])}_{os.path.splitext(os.path.basename(selected_model_path))[0]}_predicted.csv\"\n",
    "    predicted_file_dir = os.path.join(predictions_path, os.path.basename(tracking_data_path[:-4]))\n",
    "    predicted_file_path = os.path.join(predicted_file_dir, predicted_file_name)\n",
    "\n",
    "    # Create the new data directory if it doesn't exist\n",
    "    if not os.path.exists(predicted_file_dir):\n",
    "        os.makedirs(predicted_file_dir)\n",
    "    \n",
    "    new_data.to_csv(predicted_file_path, index=False)\n",
    "\n",
    "    print(f\"Predictions using {os.path.basename(selected_model_path)} saved to {predicted_file_path}\")\n",
    "\n",
    "    # Plot the new data with predictions\n",
    "    plot_movement_data(predicted_file_path, title=f\"Prediction - {os.path.basename(selected_model_path)}\")\n",
    "\n",
    "# Attach the event handler to the button\n",
    "start_analysis_button.on_click(on_analysis_button_clicked)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training (sequencial, time distributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all CSV files in the folder\n",
    "all_files = glob.glob(labeled_data_path + \"/Tracking_video*_labled.csv\")\n",
    "\n",
    "# List to store the dataframes\n",
    "li = []\n",
    "\n",
    "# Read each CSV file and add it to the list\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "# Combine all dataframes in the list into a single dataframe\n",
    "data = pd.concat(li, axis=0, ignore_index=True)\n",
    "\n",
    "# Function to create sequences from data\n",
    "def create_sequences(features, labels, window_size):\n",
    "    feature_sequences, label_sequences = [], []\n",
    "    for i in range(len(features) - window_size + 1):\n",
    "        feature_sequences.append(features.iloc[i:(i + window_size)].values)\n",
    "        label_sequences.append(labels.iloc[i + window_size - 1])\n",
    "    return np.array(feature_sequences), np.array(label_sequences)\n",
    "\n",
    "# Create an input widget for window size\n",
    "window_size_input = widgets.IntText(\n",
    "    value=4,\n",
    "    description='Frame Sequence Amount:',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'}  # Adjust the description width\n",
    ")\n",
    "display(window_size_input)\n",
    "\n",
    "# Button to start model training\n",
    "train_models_button = widgets.Button(\n",
    "    description='Train Models',\n",
    "    button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    icon='play'\n",
    ")\n",
    "display(train_models_button)\n",
    "\n",
    "def on_train_models_button_clicked(b):\n",
    "    # Use the window size from the input widget\n",
    "    window_size = window_size_input.value\n",
    "    \n",
    "    # Adjust the models directory path based on the window size\n",
    "    models_sequence_dir = f\"{models_dir}/sequence-{window_size}f\"\n",
    "    \n",
    "    # Ensure the models directory exists\n",
    "    if not os.path.exists(models_sequence_dir):\n",
    "        os.makedirs(models_sequence_dir)\n",
    "    \n",
    "    # Preparing the data with the specified window size\n",
    "    data['Left hip_y_diff'] = data['Left hip_y'].diff().fillna(0)\n",
    "    data['Right hip_y_diff'] = data['Right hip_y'].diff().fillna(0)\n",
    "    \n",
    "    X = data[['Left hip_y', 'Right hip_y']]\n",
    "    y = data['Label']\n",
    "    \n",
    "    X_seq, y_seq = create_sequences(X, y, window_size)\n",
    "    \n",
    "    X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
    "    \n",
    "    X_train_flat = X_train_seq.reshape(X_train_seq.shape[0], -1)\n",
    "    X_test_flat = X_test_seq.reshape(X_test_seq.shape[0], -1)\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_flat)\n",
    "    X_test_scaled = scaler.transform(X_test_flat)\n",
    "    \n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_train_imputed = imputer.fit_transform(X_train_scaled)\n",
    "    X_test_imputed = imputer.transform(X_test_scaled)\n",
    "    \n",
    "    models = [\n",
    "        RandomForestClassifier(),\n",
    "        LogisticRegression(),\n",
    "        SVC(),\n",
    "        DecisionTreeClassifier(),\n",
    "        GradientBoostingClassifier(),\n",
    "        KNeighborsClassifier(),\n",
    "        MLPClassifier(max_iter=1000)\n",
    "    ]\n",
    "    \n",
    "    for model in models:\n",
    "        print(f\"Training {model.__class__.__name__}...\")\n",
    "        model.fit(X_train_imputed, y_train_seq)\n",
    "        y_pred = model.predict(X_test_imputed)\n",
    "        accuracy = accuracy_score(y_test_seq, y_pred)\n",
    "        print(f\"{model.__class__.__name__} Accuracy: {accuracy}\")\n",
    "        print(\"Classification Report:\\n\", classification_report(y_test_seq, y_pred))\n",
    "        model_save_path = os.path.join(models_sequence_dir, f\"{model.__class__.__name__}.pkl\")\n",
    "        joblib.dump(model, model_save_path)\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "        print(\"--------------------------------\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "train_models_button.on_click(on_train_models_button_clicked)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming labeled_data_path is defined and points to your labeled data directory\n",
    "labeled_data_path = './data/training/labled_data'  # Update this path as necessary\n",
    "\n",
    "# UI for selecting model directory is removed as per instructions\n",
    "\n",
    "def load_and_evaluate_models():\n",
    "    print(f\"Evaluating models in sequence: {models_dropdown.value}\")  # Print which model sequence is being evaluated\n",
    "    \n",
    "# UI for selecting model directory\n",
    "models_dir = './data/models'  # Update this path as necessary\n",
    "models_dirs = [f for f in os.listdir(models_dir) if os.path.isdir(os.path.join(models_dir, f)) and f != \"old\"]\n",
    "models_dropdown = widgets.Dropdown(options=models_dirs, description='Folder:')\n",
    "display(models_dropdown)\n",
    "\n",
    "def load_and_evaluate_models(b):\n",
    "    all_files = glob.glob(labeled_data_path + \"/Tracking_video*_labled.csv\")\n",
    "    li = []\n",
    "\n",
    "    for filename in all_files:\n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        li.append(df)\n",
    "\n",
    "    data = pd.concat(li, axis=0, ignore_index=True)\n",
    "\n",
    "    # Assuming create_sequences is defined as in your training cell\n",
    "    def create_sequences(features, labels, window_size):\n",
    "        feature_sequences, label_sequences = [], []\n",
    "        for i in range(len(features) - window_size + 1):\n",
    "            feature_sequences.append(features.iloc[i:(i + window_size)].values)\n",
    "            label_sequences.append(labels.iloc[i + window_size - 1])\n",
    "        return np.array(feature_sequences), np.array(label_sequences)\n",
    "\n",
    "    # Preprocessing steps\n",
    "    data['Left hip_y_diff'] = data['Left hip_y'].diff().fillna(0)\n",
    "    data['Right hip_y_diff'] = data['Right hip_y'].diff().fillna(0)\n",
    "\n",
    "    X = data[['Left hip_y', 'Right hip_y']]\n",
    "    y = data['Label']\n",
    "\n",
    "    match = re.search(r'sequence-(\\d+)f', models_dropdown.value)\n",
    "    if match:\n",
    "        window_size = int(match.group(1))\n",
    "    else:\n",
    "        window_size = 4  # Default value if no match is found\n",
    "\n",
    "    X_seq, y_seq = create_sequences(X, y, window_size)\n",
    "\n",
    "    X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)\n",
    "\n",
    "    X_train_flat = X_train_seq.reshape(X_train_seq.shape[0], -1)\n",
    "    X_test_flat = X_test_seq.reshape(X_test_seq.shape[0], -1)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_flat)\n",
    "    X_test_scaled = scaler.transform(X_test_flat)\n",
    "\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_train_imputed = imputer.fit_transform(X_train_scaled)\n",
    "    X_test_imputed = imputer.transform(X_test_scaled)\n",
    "\n",
    "    models_sequence_dir = os.path.join(models_dir, models_dropdown.value)\n",
    "    \n",
    "    if not os.path.exists(models_sequence_dir):\n",
    "        print(f\"Directory {models_sequence_dir} does not exist. Please ensure models are trained and saved correctly.\")\n",
    "    else:\n",
    "        model_files = [f for f in os.listdir(models_sequence_dir) if f.endswith('.pkl') and f != \"old\"]\n",
    "        if not model_files:\n",
    "            print(\"No trained models found in the directory.\")\n",
    "        else:\n",
    "            for model_file in model_files:\n",
    "                model_path = os.path.join(models_sequence_dir, model_file)\n",
    "                model = joblib.load(model_path)\n",
    "                y_pred = model.predict(X_test_imputed)\n",
    "                accuracy = accuracy_score(y_test_seq, y_pred)\n",
    "                report = classification_report(y_test_seq, y_pred)\n",
    "                print(f\"Model: {model_file}\")\n",
    "                print(f\"Accuracy: {accuracy}\")\n",
    "                print(\"Classification Report:\\n\", report)\n",
    "                print(\"--------------------------------\")\n",
    "\n",
    "evaluate_button = widgets.Button(description=\"Start Analysis\", button_style='info', icon='play')\n",
    "evaluate_button.on_click(load_and_evaluate_models)\n",
    "display(evaluate_button)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
